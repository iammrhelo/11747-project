{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate import bleu_score\n",
    "import pickle as pkl\n",
    "\n",
    "\n",
    "IS_RAW = False\n",
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        self.labels = []\n",
    "        self.predictions = []\n",
    "        self.is_raw = IS_RAW\n",
    "\n",
    "\n",
    "    def add_example(self, label_output, pred_output):\n",
    "        self.labels.append(label_output)\n",
    "        self.predictions.append(pred_output)\n",
    "\n",
    "    def get_report(self):\n",
    "        # find hard accuracy\n",
    "        total_cnt = float(len(self.predictions))\n",
    "        corr_cnt = 0\n",
    "\n",
    "        # find entity precision, recall and f1\n",
    "        tp, fp, fn = 0.0, 0.0, 0.0\n",
    "\n",
    "        # find intent precision recall f1\n",
    "        itp, ifp, ifn = 0.0, 0.0, 0.0\n",
    "\n",
    "        # backend accuracy\n",
    "        btp, bfp, bfn = 0.0, 0.0, 0.0\n",
    "\n",
    "        # BLEU score\n",
    "        refs, hyps = [], []\n",
    "\n",
    "        for label, pred in zip(self.labels, self.predictions):\n",
    "            if label == pred:\n",
    "                corr_cnt += 1\n",
    "            label_ent = self._get_entities(label)\n",
    "            pred_ent = self._get_entities(pred)\n",
    "            label_backend = self._get_backend(label_ent)\n",
    "            pred_backed = self._get_backend(pred_ent)\n",
    "\n",
    "            ttpp, ffpp, ffnn = self._get_tp_fp_fn(label_ent, pred_ent)\n",
    "            tp += ttpp\n",
    "            fp += ffpp\n",
    "            fn += ffnn\n",
    "\n",
    "\n",
    "            ttpp, ffpp, ffnn = self._get_tp_fp_fn(label_backend, pred_backed)\n",
    "            btp += ttpp\n",
    "            bfp += ffpp\n",
    "            bfn += ffnn\n",
    "\n",
    "            refs.append([label.split()])\n",
    "            hyps.append(pred.split())\n",
    "\n",
    "        bleu = bleu_score.corpus_bleu(refs, hyps)\n",
    "        hard_accuracy = corr_cnt/(total_cnt+1e-20)\n",
    "\n",
    "        precision, recall, f1 = self._get_prec_recall(tp, fp, fn)\n",
    "        back_precision, back_recall, back_f1 = self._get_prec_recall(btp, bfp, bfn)\n",
    "\n",
    "        return \"Hard accuracy is %f\\n\" \\\n",
    "               \"Entity precision %f recall %f and f1 %f\\n\" \\\n",
    "               \"Backend precision %f recall %f and f1 %f\\n\" \\\n",
    "               \"BLEU %f\\n\" \\\n",
    "               % (hard_accuracy,\n",
    "                  precision, recall, f1,\n",
    "                  back_precision, back_recall, back_f1,\n",
    "                  bleu)\n",
    "\n",
    "    def _get_entities(self, sent):\n",
    "        tokens = sent.split()\n",
    "        if self.is_raw:\n",
    "            entities = []\n",
    "            buffer = []\n",
    "            for t in tokens:\n",
    "                if \"<\" in t:\n",
    "                    entities.append(t)\n",
    "                    buffer = []\n",
    "                elif t.isupper() and t != \"I\":\n",
    "                    buffer.append(t)\n",
    "                elif len(buffer) > 0:\n",
    "                    entities.append(\" \".join(buffer))\n",
    "                    buffer = []\n",
    "            if len(buffer) > 0:\n",
    "                entities.append(\" \".join(buffer))\n",
    "            # check for times\n",
    "            for t in tokens:\n",
    "                if t.isdigit():\n",
    "                    entities.append(t)\n",
    "                if \"a.m\" in t or \"p.m\" in t:\n",
    "                    entities.append(t)\n",
    "        else:\n",
    "            entities = [t for t in tokens if \"<\" in t]\n",
    "        return entities\n",
    "\n",
    "    def _get_tp_fp_fn(self, label_ents, pred_ents):\n",
    "        tp = len([t for t in pred_ents if t in label_ents])\n",
    "        fp = len(pred_ents) - tp\n",
    "        fn = len(label_ents) - tp\n",
    "        return tp, fp, fn\n",
    "\n",
    "    def _get_backend(self, entities):\n",
    "        for t in entities:\n",
    "            if \"<backend\" in t:\n",
    "                return entities\n",
    "        return []\n",
    "\n",
    "    def _get_prec_recall(self, tp, fp, fn):\n",
    "        precision = tp / (tp + fp + 10e-20)\n",
    "        recall = tp / (tp + fn + 10e-20)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-20)\n",
    "        return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.txt', 'r') as f:\n",
    "    out = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('truth.txt', 'r') as f:\n",
    "    tru = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = pkl.load( open( \"test_idx.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = list(zip(test_idx, out))\n",
    "ret.sort(key=lambda tup: tup[0])\n",
    "ret = [each[1] for each in ret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, each in enumerate(ret):\n",
    "    ev.add_example(tru[i], each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard accuracy is 0.111306\n",
      "Entity precision 0.177019 recall 0.189133 and f1 0.182875\n",
      "Backend precision 0.000000 recall 0.000000 and f1 0.000000\n",
      "BLEU 0.380378\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ev.get_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
